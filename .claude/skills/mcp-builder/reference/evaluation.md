# MCP 服务器评估指南

## 概述

本文档提供了为 MCP 服务器创建全面评估的指导。评估测试 LLM 是否能仅使用提供的工具有效使用你的 MCP 服务器来回答现实、复杂的问题。

---

## 快速参考

### 评估要求
- 创建 10 个人类可读的问题
- 问题必须是只读的、独立的、非破坏性的
- 每个问题需要多个工具调用（可能数十个）
- 答案必须是单一、可验证的值
- 答案必须是稳定的（不会随时间变化）

### 输出格式
```xml
<evaluation>
   <qa_pair>
      <question>你的问题在这里</question>
      <answer>单一可验证答案</answer>
   </qa_pair>
</evaluation>
```

---

## 评估的目的

MCP 服务器的质量衡量标准不是服务器实现工具的好坏或全面程度，而是这些实现（输入/输出模式、文档字符串/描述、功能）如何在没有其他上下文且只能访问 MCP 服务器的情况下，使 LLM 能够回答现实且困难的问题。

## 评估概述

创建 10 个人类可读的问题，仅需要只读、独立、非破坏性和幂等操作来回答。每个问题应该：
- 现实的
- 清晰简洁的
- 无歧义的
- 复杂的，可能需要几十个工具调用或步骤
- 可以用你提前识别的单一、可验证值回答

## 问题指导原则

### 核心要求

1. **问题必须是独立的**
   - 每个问题不应依赖于任何其他问题的答案
   - 不应假设处理另一个问题时的先写操作

2. **问题必须只需要非破坏性和幂等工具使用**
   - 不应指示或要求修改状态以获得正确答案

3. **问题必须是现实的、清晰的、简洁的和复杂的**
   - 必须要求另一个 LLM 使用多个（可能几十个）工具或步骤来回答

### 复杂性和深度

4. **问题必须需要深度探索**
   - 考虑需要多个子问题和顺序工具调用的多跳问题
   - 每一步都应该受益于在之前问题中发现的信息

5. **问题可能需要大量分页**
   - 可能需要分页浏览多个结果页面
   - 可能需要查询旧数据（1-2 年前的）以找到小众信息
   - 问题必须是困难的

6. **问题必须需要深度理解**
   - 而不是表面知识
   - 可能将复杂思想提出为需要证据的真/假问题
   - 可能使用多项选择格式，其中 LLM 必须搜索不同的假设

7. **问题不能通过直接关键词搜索解决**
   - 不要包含目标内容中的特定关键词
   - 使用同义词、相关概念或转述
   - 需要多个搜索、分析多个相关项目、提取上下文，然后推导答案

### 工具测试

8. **问题应该对工具返回值进行压力测试**
   - 可能引发工具返回大型 JSON 对象或列表，压垮 LLM
   - 应该需要理解多种数据模式：
     - ID 和名称
     - 时间戳和日期时间（月、日、年、秒）
     - 文件 ID、名称、扩展名和 mime 类型
     - URL、GID 等
   - 应该探查工具返回所有有用形式数据的能力

9. **问题应该主要反映真实的人类用例**
   - LLM 辅助的人类会关心的信息检索任务类型

10. **问题可能需要几十个工具调用**
    - 这对具有有限上下文的 LLM 构成挑战
    - 鼓励 MCP 服务器工具减少返回的信息

11. **包含模糊问题**
    - 可能是模糊的或需要关于调用哪些工具的困难决策
    - 强迫 LLM 可能犯错或误解
    - 确保尽管存在模糊性，仍然有单一可验证答案

### 稳定性

12. **问题必须设计成答案不会改变**
    - 不要问依赖于"当前状态"的问题，这是动态的
    - 例如，不要计算：
      - 帖子的反应数
      - 线程的回复数
      - 频道中的成员数

13. **不要让 MCP 服务器限制你创建的问题类型**
    - 创建具有挑战性和复杂性的问题
    - 有些可能无法用可用的 MCP 服务器工具解决
    - 问题可能需要特定的输出格式（datetime vs epoch time，JSON vs MARKDOWN）
    - 问题可能需要几十个工具调用才能完成

## 答案指导原则

### 验证

1. **答案必须可通过直接字符串比较验证**
   - 如果答案可以用多种格式重写，在问题中明确指定输出格式
   - 示例："使用 YYYY/MM/DD。"，"回答 True 或 False。"，"回答 A、B、C 或 D，仅此而已。"
   - 答案应该是单一可验证值，如：
     - 用户 ID、用户名、显示名、名字、姓氏
     - 频道 ID、频道名称
     - 消息 ID、字符串
     - URL、标题
     - 数值
     - 时间戳、日期时间
     - 布尔值（对于真/假问题）
     - 电子邮件地址、电话号码
     - 文件 ID、文件名、文件扩展名
     - 多项选择答案
   - 答案不需要特殊格式或复杂的结构化输出
   - 答案将使用直接字符串比较验证

### 可读性

2. **答案通常应该偏好人类可读的格式**
   - 示例：名称、名字、姓氏、日期时间、文件名、消息字符串、URL、是/否、真/假、a/b/c/d
   - 而不是不透明的 ID（尽管 ID 是可接受的）
   - 绝大多数答案应该是人类可读的

### 稳定性

3. **答案必须是稳定/静止的**
    - 查看旧内容（例如，已结束的对话、已启动的项目、已回答的问题）
    - 基于"关闭"概念创建问题，这些概念将始终返回相同答案
    - 问题可能要求考虑固定时间窗口以避免非静止答案
    - 依赖于不太可能改变的上下文
    - 示例：如果查找论文名称，要足够具体，以免与后来发表的论文混淆

4. **答案必须是清晰和无歧义的**
    - 问题必须设计成有单一、清晰的答案
    - 答案可以从使用 MCP 服务器工具推导出来

### 多样性

5. **答案必须是多样的**
   - 答案应该是单一可验证值，具有多种模式和格式
   - 用户概念：用户 ID、用户名、显示名、名字、姓氏、电子邮件地址、电话号码
   - 频道概念：频道 ID、频道名称、频道主题
   - 消息概念：消息 ID、消息字符串、时间戳、月、日、年

6. **答案不能是复杂结构**
   - 不是值列表
   - 不是复杂对象
   - 不是 ID 或字符串列表
   - 不是自然语言文本
   - 除非答案可以通过直接字符串比较直接验证
   - 并且可以现实地重现
   - LLM 不太可能以任何其他顺序或格式返回相同列表的可能性应该很小

## 评估流程

### 第 1 步：文档检查

阅读目标 API 的文档以了解：
- 可用的端点和功能
- 如果存在模糊性，从网络获取额外信息
- 尽可能并行化此步骤
- 确保每个子代理只检查文件系统或网络上的文档

### 第 2 步：工具检查

列出 MCP 服务器中可用的工具：
- 直接检查 MCP 服务器
- 理解输入/输出模式、文档字符串和描述
- 在此阶段不要调用工具本身

### 第 3 步：发展理解

重复步骤 1 和 2，直到你有良好的理解：
- 多次迭代
- 考虑你想要创建的任务类型
- 完善你的理解
- 在任何阶段都不应阅读 MCP 服务器实现本身的代码
- 使用你的直觉和理解来创建合理、现实但非常具有挑战性的任务

### 第 4 步：只读内容检查

理解 API 和工具后，使用 MCP 服务器工具：
- 仅使用只读和非破坏性操作检查内容
- 目标：识别特定内容（例如，用户、频道、消息、项目、任务）以创建现实问题
- 不应调用任何修改状态的工具
- 不会阅读 MCP 服务器实现本身的代码
- 与追求独立探索的各个子代理并行化此步骤
- 确保每个子代理只执行只读、非破坏性和幂等操作
- 注意：一些工具可能返回大量数据，这会使你超出上下文限制
- 进行增量、小而有针对性的工具调用进行探索
- 在所有工具调用请求中，使用 `limit` 参数限制结果（<10）
- 使用分页

### 第 5 步：任务生成

检查内容后，创建 10 个人类可读的问题：
- LLM 应该能够使用 MCP 服务器回答这些问题
- 遵循上述所有问题和答案指导原则

## 输出格式

每个 QA 对由一个问题和一个答案组成。输出应该是具有以下结构的 XML 文件：

```xml
<evaluation>
   <qa_pair>
      <question>查找在 2024 年第二季度创建的具有最高已完成任务数的项目的项目名称。</question>
      <answer>网站重新设计</answer>
   </qa_pair>
   <qa_pair>
      <question>搜索标记为"bug"且在 2024 年 3 月关闭的问题。哪个用户关闭了最多问题？提供他们的用户名。</question>
      <answer>sarah_dev</answer>
   </qa_pair>
   <qa_pair>
      <question>查找修改了 /api 目录中的文件并在 2024 年 1 月 1 日至 2024 年 1 月 31 日之间合并的拉取请求。有多少不同的贡献者处理了这些 PR？</question>
      <answer>7</answer>
   </qa_pair>
   <qa_pair>
      <question>查找在 2023 年之前创建的星数最多的存储库。存储库名称是什么？</question>
      <answer>data-pipeline</answer>
   </qa_pair>
</evaluation>
```

## 评估示例

### 好的问题

**示例 1：需要深度探索的多跳问题（GitHub MCP）**
```xml
<qa_pair>
   <question>查找在 2023 年第三季度归档且之前是组织中最多被分叉项目的存储库。该存储库中使用的编程语言是什么？</question>
   <answer>Python</answer>
</qa_pair>
```

这个问题是好的，因为：
- 需要多次搜索来找到归档的存储库
- 需要识别在归档前哪个具有最多分叉
- 需要检查存储库详细信息以了解语言
- 答案是简单、可验证的值
- 基于历史（已关闭）数据，不会改变

**示例 2：需要在没有关键词匹配的情况下理解上下文（项目管理 MCP）**
```xml
<qa_pair>
   <question>定位在 2023 年底完成的专注于改善客户入职的倡议。项目主管在完成后创建了一个回顾文档。主管当时的角色头衔是什么？</question>
   <answer>产品经理</answer>
</qa_pair>
```

这个问题是好的，因为：
- 不使用特定项目名称（"专注于改善客户入职的倡议"）
- 需要从特定时间范围查找已完成项目
- 需要识别项目主管和他们的角色
- 需要从回顾文档理解上下文
- 答案是人类可读且稳定的
- 基于已完成的工作（不会改变）

**示例 3：需要多个步骤的复杂聚合（问题跟踪器 MCP）**
```xml
<qa_pair>
   <question>在 2024 年 1 月报告并标记为关键优先级的所有错误中，哪个受让人在 48 小时内解决了其分配错误的最高百分比？提供受让人的用户名。</question>
   <answer>alex_eng</answer>
</qa_pair>
```

这个问题是好的，因为：
- 需要按日期、优先级和状态过滤错误
- 需要按受让人分组并计算解决率
- 需要理解时间戳以确定 48 小时窗口
- 测试分页（可能需要处理许多错误）
- 答案是单一用户名
- 基于特定时间段的历史数据

**示例 4：需要跨多种数据类型综合（CRM MCP）**
```xml
<qa_pair>
   <question>查找在 2023 年第四季度从 Starter 升级到 Enterprise 计划且具有最高年度合同价值的账户。该账户在哪个行业运营？</question>
   <answer>医疗保健</answer>
</qa_pair>
```

这个问题是好的，因为：
- 需要理解订阅级别变化
- 需要识别特定时间范围内的升级事件
- 需要比较合同价值
- 必须访问账户行业信息
- 答案简单且可验证
- 基于已完成的历史交易

### 差的问题

**示例 1：答案随时间变化**
```xml
<qa_pair>
   <question>当前分配给工程团队有多少个开放问题？</question>
   <answer>47</answer>
</qa_pair>
```

这个问题是差的，因为：
- 答案会随着问题被创建、关闭或重新分配而改变
- 不基于稳定/静止数据
- 依赖于动态的"当前状态"

**示例 2：关键词搜索太容易**
```xml
<qa_pair>
   <question>查找标题为"Add authentication feature"的拉取请求，告诉我谁创建了它。</question>
   <answer>developer123</answer>
</qa_pair>
```

这个问题是差的，因为：
- 可以通过对确切标题的简单关键词搜索解决
- 不需要深度探索或理解
- 不需要综合或分析

**示例 3：答案格式模糊**
```xml
<qa_pair>
   <question>列出所有以 Python 为主要语言的存储库。</question>
   <answer>repo1, repo2, repo3, data-pipeline, ml-tools</answer>
</qa_pair>
```

这个问题是差的，因为：
- 答案是可能以任何顺序返回的列表
- 难以通过直接字符串比较验证
- LLM 可能格式不同（JSON 数组、逗号分隔、换行符分隔）
- 更好的方式是询问特定聚合（计数）或最高级（最多星数）

## 验证流程

创建评估后：

1. **检查 XML 文件**以了解模式
2. **加载每个任务指令**并使用 MCP 服务器和工具，通过尝试自己解决任务来识别正确答案
3. **标记任何需要写入或破坏性操作的操作**
4. **累积所有正确答案**并替换文档中任何不正确的答案
5. **删除任何 `<qa_pair>`**需要写入或破坏性操作

记住要并行化解决任务以避免超出上下文，然后累积所有答案并在最后对文件进行更改。

## 创建质量评估的技巧

1. **努力思考并提前规划**然后再生成任务
2. **在机会出现时并行化**以加速流程和管理上下文
3. **专注于现实用例**，这些是人类实际想要完成的
4. **创建具有挑战性的问题**，测试 MCP 服务器能力的极限
5. **通过使用历史数据和关闭概念确保稳定性**
6. **通过自己使用 MCP 服务器工具解决问题来验证答案**
7. **根据你在过程中学到的东西迭代和完善**

---

# 运行评估

创建评估文件后，你可以使用提供的评估工具来测试你的 MCP 服务器。

## 设置

1. **安装依赖项**

   ```bash
   pip install -r scripts/requirements.txt
   ```

   或手动安装：
   ```bash
   pip install anthropic mcp
   ```

2. **设置 API 密钥**

   ```bash
   export ANTHROPIC_API_KEY=your_api_key_here
   ```

## 评估文件格式

评估文件使用带有 `<qa_pair>` 元素的 XML 格式：

```xml
<evaluation>
   <qa_pair>
      <question>查找在 2024 年第二季度创建的具有最高已完成任务数的项目的项目名称。</question>
      <answer>网站重新设计</answer>
   </qa_pair>
   <qa_pair>
      <question>搜索标记为"bug"且在 2024 年 3 月关闭的问题。哪个用户关闭了最多问题？提供他们的用户名。</question>
      <answer>sarah_dev</answer>
   </qa_pair>
</evaluation>
```

## 运行评估

评估脚本（`scripts/evaluation.py`）支持三种传输类型：

**重要说明：**
- **stdio 传输**：评估脚本自动为你启动和管理 MCP 服务器进程。不要手动运行服务器。
- **sse/http 传输**：在运行评估前必须单独启动 MCP 服务器。脚本连接到指定 URL 上已运行的服务器。

### 1. 本地 STDIO 服务器

对于本地运行的 MCP 服务器（脚本自动启动服务器）：

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_mcp_server.py \
  evaluation.xml
```

使用环境变量：
```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_mcp_server.py \
  -e API_KEY=abc123 \
  -e DEBUG=true \
  evaluation.xml
```

### 2. 服务器发送事件 (SSE)

对于基于 SSE 的 MCP 服务器（必须先启动服务器）：

```bash
python scripts/evaluation.py \
  -t sse \
  -u https://example.com/mcp \
  -H "Authorization: Bearer token123" \
  -H "X-Custom-Header: value" \
  evaluation.xml
```

### 3. HTTP（可流式 HTTP）

对于基于 HTTP 的 MCP 服务器（必须先启动服务器）：

```bash
python scripts/evaluation.py \
  -t http \
  -u https://example.com/mcp \
  -H "Authorization: Bearer token123" \
  evaluation.xml
```

## 命令行选项

```
usage: evaluation.py [-h] [-t {stdio,sse,http}] [-m MODEL] [-c COMMAND]
                     [-a ARGS [ARGS ...]] [-e ENV [ENV ...]] [-u URL]
                     [-H HEADERS [HEADERS ...]] [-o OUTPUT]
                     eval_file

positional arguments:
  eval_file             评估 XML 文件的路径

optional arguments:
  -h, --help            显示帮助消息
  -t, --transport       传输类型：stdio、sse 或 http（默认：stdio）
  -m, --model           要使用的 Claude 模型（默认：claude-3-7-sonnet-20250219）
  -o, --output          报告的输出文件（默认：打印到 stdout）

stdio 选项:
  -c, --command         运行 MCP 服务器的命令（例如，python、node）
  -a, --args            命令的参数（例如，server.py）
  -e, --env             格式为 KEY=VALUE 的环境变量

sse/http 选项:
  -u, --url             MCP 服务器 URL
  -H, --header          格式为 'Key: Value' 的 HTTP 头
```

## 输出

评估脚本生成详细的报告，包括：

- **摘要统计**：
  - 准确性（正确/总数）
  - 平均任务持续时间
  - 每个任务的平均工具调用
  - 总工具调用

- **每个任务结果**：
  - 提示和预期响应
  - 来自代理的实际响应
  - 答案是否正确（✅/❌）
  - 持续时间和工具调用详细信息
  - 代理对其方法的总结
  - 代理对工具的反馈

### 将报告保存到文件

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_server.py \
  -o evaluation_report.md \
  evaluation.xml
```

## 完整示例工作流程

以下是创建和运行评估的完整示例：

1. **创建你的评估文件**（`my_evaluation.xml`）：

```xml
<evaluation>
   <qa_pair>
      <question>查找在 2024 年 1 月创建最多问题的用户。他们的用户名是什么？</question>
      <answer>alice_developer</answer>
   </qa_pair>
   <qa_pair>
      <question>在 2024 年第一季度合并的所有拉取请求中，哪个存储库的数量最多？提供存储库名称。</question>
      <answer>backend-api</answer>
   </qa_pair>
   <qa_pair>
      <question>查找在 2023 年 12 月完成且从开始到结束持续时间最长的项目。它花费了多少天？</question>
      <answer>127</answer>
   </qa_pair>
</evaluation>
```

2. **安装依赖项**：

```bash
pip install -r scripts/requirements.txt
export ANTHROPIC_API_KEY=your_api_key
```

3. **运行评估**：

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a github_mcp_server.py \
  -e GITHUB_TOKEN=ghp_xxx \
  -o github_eval_report.md \
  my_evaluation.xml
```

4. **查看报告**在 `github_eval_report.md` 中：
   - 查看哪些问题通过/失败
   - 阅读代理对你的工具的反馈
   - 识别改进领域
   - 迭代你的 MCP 服务器设计

## 故障排除

### 连接错误

如果遇到连接错误：
- **STDIO**：验证命令和参数是否正确
- **SSE/HTTP**：检查 URL 是否可访问且头是否正确
- 确保在环境变量或头中设置任何所需的 API 密钥

### 准确性低

如果许多评估失败：
- 查看每个任务的代理反馈
- 检查工具描述是否清晰全面
- 验证输入参数是否记录良好
- 考虑工具是否返回过多或过少的数据
- 确保错误消息是可操作的

### 超时问题

如果任务超时：
- 使用更有能力的模型（例如，`claude-3-7-sonnet-20250219`）
- 检查工具是否返回过多数据
- 验证分页是否正常工作
- 考虑简化复杂问题